{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/brittabeta/Data-Engineering-Duke/blob/main/persistence.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k9JHm-G7KaHg"
      },
      "source": [
        "# Persistence and Efficency\n",
        "persist the collected data, so that it can be analyzed and cleaned later - \n",
        "parsing, retrieving, saving, and cleaning data are all separate actions - do NOT try to work with data while collecting. \n",
        "\n",
        "* parsing techniques\n",
        "* persistency: JSON, CSV, SQL and a database\n",
        "\n",
        "load files and `scrapy` library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BqqwEF23KaHi",
        "outputId": "71d3f247-6627-4796-8c40-c87f67d580af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: Command '['/content/venv/bin/python3', '-m', 'ensurepip', '--upgrade', '--default-pip']' returned non-zero exit status 1.\n",
            "/bin/bash: venv/bin/activate: No such file or directory\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting scrapy==2.5.1\n",
            "  Downloading Scrapy-2.5.1-py2.py3-none-any.whl (254 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m255.0/255.0 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting protego>=0.1.15\n",
            "  Downloading Protego-0.2.1-py2.py3-none-any.whl (8.2 kB)\n",
            "Collecting itemloaders>=1.0.1\n",
            "  Downloading itemloaders-1.1.0-py3-none-any.whl (11 kB)\n",
            "Collecting pyOpenSSL>=16.2.0\n",
            "  Downloading pyOpenSSL-23.1.1-py3-none-any.whl (57 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.9/57.9 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h2<4.0,>=3.0\n",
            "  Downloading h2-3.2.0-py2.py3-none-any.whl (65 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.0/65.0 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: lxml>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from scrapy==2.5.1->-r requirements.txt (line 1)) (4.9.2)\n",
            "Collecting itemadapter>=0.1.0\n",
            "  Downloading itemadapter-0.8.0-py3-none-any.whl (11 kB)\n",
            "Collecting parsel>=1.5.0\n",
            "  Downloading parsel-1.8.1-py2.py3-none-any.whl (17 kB)\n",
            "Collecting PyDispatcher>=2.0.5\n",
            "  Downloading PyDispatcher-2.0.7-py3-none-any.whl (12 kB)\n",
            "Collecting w3lib>=1.17.0\n",
            "  Downloading w3lib-2.1.1-py3-none-any.whl (21 kB)\n",
            "Collecting Twisted[http2]>=17.9.0\n",
            "  Downloading Twisted-22.10.0-py3-none-any.whl (3.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m50.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting service-identity>=16.0.0\n",
            "  Downloading service_identity-21.1.0-py2.py3-none-any.whl (12 kB)\n",
            "Collecting cssselect>=0.9.1\n",
            "  Downloading cssselect-1.2.0-py2.py3-none-any.whl (18 kB)\n",
            "Collecting queuelib>=1.4.2\n",
            "  Downloading queuelib-1.6.2-py2.py3-none-any.whl (13 kB)\n",
            "Collecting zope.interface>=4.1.3\n",
            "  Downloading zope.interface-6.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (246 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m247.0/247.0 kB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: cryptography>=2.0 in /usr/local/lib/python3.10/dist-packages (from scrapy==2.5.1->-r requirements.txt (line 1)) (40.0.2)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=2.0->scrapy==2.5.1->-r requirements.txt (line 1)) (1.15.1)\n",
            "Collecting hyperframe<6,>=5.2.0\n",
            "  Downloading hyperframe-5.2.0-py2.py3-none-any.whl (12 kB)\n",
            "Collecting hpack<4,>=3.0\n",
            "  Downloading hpack-3.0.0-py2.py3-none-any.whl (38 kB)\n",
            "Collecting jmespath>=0.9.5\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from parsel>=1.5.0->scrapy==2.5.1->-r requirements.txt (line 1)) (23.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from protego>=0.1.15->scrapy==2.5.1->-r requirements.txt (line 1)) (1.16.0)\n",
            "Requirement already satisfied: pyasn1-modules in /usr/local/lib/python3.10/dist-packages (from service-identity>=16.0.0->scrapy==2.5.1->-r requirements.txt (line 1)) (0.3.0)\n",
            "Requirement already satisfied: pyasn1 in /usr/local/lib/python3.10/dist-packages (from service-identity>=16.0.0->scrapy==2.5.1->-r requirements.txt (line 1)) (0.5.0)\n",
            "Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.10/dist-packages (from service-identity>=16.0.0->scrapy==2.5.1->-r requirements.txt (line 1)) (23.1.0)\n",
            "Collecting constantly>=15.1\n",
            "  Downloading constantly-15.1.0-py2.py3-none-any.whl (7.9 kB)\n",
            "Requirement already satisfied: typing-extensions>=3.6.5 in /usr/local/lib/python3.10/dist-packages (from Twisted[http2]>=17.9.0->scrapy==2.5.1->-r requirements.txt (line 1)) (4.5.0)\n",
            "Collecting incremental>=21.3.0\n",
            "  Downloading incremental-22.10.0-py2.py3-none-any.whl (16 kB)\n",
            "Collecting Automat>=0.8.0\n",
            "  Downloading Automat-22.10.0-py2.py3-none-any.whl (26 kB)\n",
            "Collecting hyperlink>=17.1.1\n",
            "  Downloading hyperlink-21.0.0-py2.py3-none-any.whl (74 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.6/74.6 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting priority<2.0,>=1.1.0\n",
            "  Downloading priority-1.3.0-py2.py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from zope.interface>=4.1.3->scrapy==2.5.1->-r requirements.txt (line 1)) (67.7.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=2.0->scrapy==2.5.1->-r requirements.txt (line 1)) (2.21)\n",
            "Requirement already satisfied: idna>=2.5 in /usr/local/lib/python3.10/dist-packages (from hyperlink>=17.1.1->Twisted[http2]>=17.9.0->scrapy==2.5.1->-r requirements.txt (line 1)) (3.4)\n",
            "Installing collected packages: PyDispatcher, priority, incremental, hyperframe, hpack, constantly, zope.interface, w3lib, queuelib, protego, jmespath, itemadapter, hyperlink, h2, cssselect, Automat, Twisted, parsel, service-identity, pyOpenSSL, itemloaders, scrapy\n",
            "Successfully installed Automat-22.10.0 PyDispatcher-2.0.7 Twisted-22.10.0 constantly-15.1.0 cssselect-1.2.0 h2-3.2.0 hpack-3.0.0 hyperframe-5.2.0 hyperlink-21.0.0 incremental-22.10.0 itemadapter-0.8.0 itemloaders-1.1.0 jmespath-1.0.1 parsel-1.8.1 priority-1.3.0 protego-0.2.1 pyOpenSSL-23.1.1 queuelib-1.6.2 scrapy-2.5.1 service-identity-21.1.0 w3lib-2.1.1 zope.interface-6.0\n"
          ]
        }
      ],
      "source": [
        "# create an activated a virtual environment \n",
        "!python3 -m venv venv\n",
        "!source venv/bin/activate\n",
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "7Nrn4JJlKaHi"
      },
      "outputs": [],
      "source": [
        "import scrapy\n",
        "import os\n",
        "current_dir = os.path.abspath('')\n",
        "url = os.path.join(current_dir, \"/content/1992_World_Junior_Championships_in_Athletics_–_Men's_high_jump\")\n",
        "with open(url) as _f:\n",
        "    url_data = _f.read()\n",
        "\n",
        "response = scrapy.http.TextResponse(url, body=url_data, encoding='utf-8')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ia25abP-KaHj",
        "outputId": "f53d6d4c-3d0c-4b3b-89ba-138b3e112b3d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gold Steve Smith\n",
            "Silver Tim Forsyth\n",
            "Bronze Takahiro Kimino\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/scrapy/selector/unified.py:82: UserWarning: Selector got both text and root, root is being ignored.\n",
            "  super().__init__(text=text, type=st, root=root, **kwargs)\n"
          ]
        }
      ],
      "source": [
        "# interesting data is available ?\n",
        "table = response.xpath('//table')[1].xpath('tbody')\n",
        "for tr in table.xpath('tr'):\n",
        "    print(tr.xpath('td/b/text()').extract()[0],\n",
        "          tr.xpath('td/a/text()').extract()[0]\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kd-qjWCkKaHk"
      },
      "source": [
        "interaction with `scrapy` in a Notebook is useful:\n",
        "* don't need to run the special shell \n",
        "* don't need to run the whole spider\n",
        "* good testing env to adapt spider \n",
        "\n",
        "persisting with JSON\n",
        "* keep the information in a Python data structure \n",
        "* dictionary\n",
        "* load it as a JSON object\n",
        "* save it to a file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bkxCMJ95KaHk",
        "outputId": "86563f91-dbd2-4703-bcfd-c0b027b3de9c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Gold': 'Steve Smith', 'Silver': 'Tim Forsyth', 'Bronze': 'Takahiro Kimino'}"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "scrapped_data = {}\n",
        "for tr in table.xpath('tr'):\n",
        "    medal = tr.xpath('td/b/text()').extract()[0]\n",
        "    athlete = tr.xpath('td/a/text()').extract()[0]\n",
        "    scrapped_data[medal] = athlete\n",
        "\n",
        "scrapped_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "vzuPJXR5KaHk"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "#json_data = json.dumps(scrapped_data)\n",
        "\n",
        "# persist\n",
        "with open(\"1992_results.json\", \"w\") as _f:\n",
        "    # use dump() with the Python dictionary directly. \n",
        "    # the conversion is done on the fly\n",
        "    json.dump(scrapped_data, _f)\n",
        "\n",
        "    # creates file /content/1992_results.json with key:value"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mOIhiwnYKaHk"
      },
      "source": [
        "persist with CSV\n",
        "* can use Pandas\n",
        "* can also use the standard library CSV module"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "5HXRhXGgKaHl"
      },
      "outputs": [],
      "source": [
        "# construct the data first\n",
        "\n",
        "column_names = [\"Medal\", \"Athlete\"]\n",
        "rows = []\n",
        "\n",
        "for tr in table.xpath('tr'):\n",
        "    medal = tr.xpath('td/b/text()').extract()[0]\n",
        "    athlete = tr.xpath('td/a/text()').extract()[0]\n",
        "    rows.append([medal, athlete])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "u4d83R1bKaHl"
      },
      "outputs": [],
      "source": [
        "# persist it to disk\n",
        "import csv\n",
        "\n",
        "with open(\"1992_results.csv\", \"w\") as _f:\n",
        "    writer = csv.writer(_f)\n",
        "\n",
        "    # write the column names\n",
        "    writer.writerow(column_names)\n",
        "\n",
        "    # now write the rows\n",
        "    writer.writerows(rows)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mvmIHTm3KaHl"
      },
      "source": [
        "persist to a database:\n",
        "* much more memory efficient\n",
        "* can save the data as the data is gathered \n",
        "* SQLite database to persist the data\n",
        "* file-based database and the table needed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "5EvBh-SrKaHl"
      },
      "outputs": [],
      "source": [
        "import sqlite3\n",
        "connection = sqlite3.connect(\"1992_results.db\")\n",
        "db_table = 'CREATE TABLE results (id integer primary key, medal TEXT, athlete TEXT)'\n",
        "cursor = connection.cursor()\n",
        "cursor.execute(db_table)\n",
        "connection.commit()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "Zbw3ieRNKaHm"
      },
      "outputs": [],
      "source": [
        "# persist the data\n",
        "connection = sqlite3.connect(\"1992_results.db\")\n",
        "cursor = connection.cursor()\n",
        "query = 'INSERT INTO results(medal, athlete) VALUES(?, ?)'\n",
        "\n",
        "for tr in table.xpath('tr'):\n",
        "    medal = tr.xpath('td/b/text()').extract()[0]\n",
        "    athlete = tr.xpath('td/a/text()').extract()[0]\n",
        "    cursor.execute(query, (medal, athlete)) \n",
        "    connection.commit()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# connect and query\n",
        "db = \"/content/1992_results.db\"\n",
        "connection = sqlite3.connect(db)\n",
        "cursor = connection.cursor()\n",
        "query = 'SELECT * FROM results'\n",
        "cursor.execute(query)\n",
        "cursor.fetchall() # or fetchone()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0YLNiE8pPQTu",
        "outputId": "836563b0-e5fc-4620-e740-a4d9a1fea806"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(1, 'Gold', 'Steve Smith'),\n",
              " (2, 'Silver', 'Tim Forsyth'),\n",
              " (3, 'Bronze', 'Takahiro Kimino')]"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# close\n",
        "connection.close()"
      ],
      "metadata": {
        "id": "AVzpeip2Qxug"
      },
      "execution_count": 25,
      "outputs": []
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "0303190f1c8c691fa9994d3c799a212d90e80d675371cad4184da4200e89748e"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}